Multi-Agent Ticket Routing
user
Iâ€™m building a multi-agent system for a case study.
The goal is to analyze customer support tickets and route them appropriately.
I want to use 2â€“3 agents using the Pydantic AI pattern, one for sentiment, one for customer priority, and maybe one for routing.
How should I structure this architecture so that each agent is specialized but they can also work together?
ChatGPT
Great idea â€” building a modular multi-agent system with Pydantic AI pattern will help you keep each agentâ€™s role clear while allowing them to interoperate. Hereâ€™s a solid architecture to start with:

---

## ðŸ§  **Multi-Agent Architecture Overview**

### ðŸ—ï¸ Key Components
1. **Agent 1 â€“ Sentiment Analyzer**
   - Extracts sentiment (positive, neutral, negative) from the support ticket text.
   - Output: `{sentiment: "positive" | "neutral" | "negative"}`

2. **Agent 2 â€“ Customer Priority Classifier**
   - Classifies customer priority based on metadata like customer tier, past history, ticket urgency.
   - Output: `{priority: "high" | "medium" | "low"}`

3. **Agent 3 â€“ Router Agent**
   - Takes input from Agent 1 and Agent 2 and decides the correct support team or queue.
   - Output: `{route: "technical" | "billing" | "retention" | etc.}`

4. **Orchestrator (optional)**
   - A central controller or flow engine that manages agent execution and data passing.
   - Helps scale to more agents later and allows async execution.

---

## ðŸ§© **Pydantic AI Pattern**
Each agent is structured as a Pydantic model that:
- Defines its **input schema**
- Validates and processes input
- Produces a structured **output schema**

### Example: Sentiment Agent
```python
from pydantic import BaseModel

class SentimentInput(BaseModel):
    ticket_text: str

class SentimentOutput(BaseModel):
    sentiment: str  # "positive", "neutral", "negative"

class SentimentAgent:
    def run(self, input: SentimentInput) -> SentimentOutput:
        # insert model or rule-based logic here
        sentiment = some_sentiment_analysis_function(input.ticket_text)
        return SentimentOutput(sentiment=sentiment)
```

Repeat this format for each agent. This keeps each component testable and clear.

---

## ðŸ”— **Agent Collaboration Flow**

```text
[TICKET] 
   â†“
[Sentiment Agent] ---> Output A
[Priority Agent]  ---> Output B
   â†“
   [Routing Agent]
   Receives A + B + metadata
   â†“
â†’ Final routing decision
```

You can structure the system with a pipeline controller (orchestrator):

```python
class Orchestrator:
    def __init__(self):
        self.sentiment_agent = SentimentAgent()
        self.priority_agent = PriorityAgent()
        self.routing_agent = RoutingAgent()

    def run(self, ticket_text, metadata):
        sentiment = self.sentiment_agent.run(SentimentInput(ticket_text=ticket_text))
        priority = self.priority_agent.run(PriorityInput(metadata=metadata))
        route = self.routing_agent.run(RoutingInput(
            ticket_text=ticket_text,
            sentiment=sentiment.sentiment,
            priority=priority.priority,
            metadata=metadata
        ))
        return route
```

---

## ðŸ§¬ Best Practices

- **Use dependency injection** for easier testing of each agent.
- **Make outputs immutable (frozen models)** to avoid accidental mutation downstream.
- Use **async agents** if any of them call APIs or large models.
- **Add a logging layer** to trace what each agent is doing.

---

## ðŸ§ª Bonus: Extendibility

Once this is in place, you can easily add more agents:
- **Language Detector Agent** for multilingual support
- **Topic Tagger Agent** for intent classification
- **Escalation Predictor** for automatic red flags

These can plug into the same orchestrator pipeline with minimal disruption.

---

Let me know if you want a real code scaffold for this setup â€” I can give you the full orchestrator and sample agent implementations.
user
That makes sense. Can we simulate this with mock logic for now instead of actual LLMs?
Iâ€™m thinking Iâ€™ll define each agentâ€™s input and output models using BaseModel, and just use fake data for the outputs. Is that a good idea for the prototype?
ChatGPT
Yes â€” **thatâ€™s a perfect approach for prototyping**.

Simulating with mock logic while using real `Pydantic` models for input/output helps you:

- Nail down your agent interfaces
- Validate the data flow between components
- Test the orchestration logic early
- Swap in real LLMs or model logic later with zero or minimal refactoring

---

## âœ… Why This Works Well

- **Real structure, fake logic** â€” Pydantic ensures your interfaces are solid while you iterate fast.
- **Testability** â€” You can write unit tests for each agent's "contract" even with mock logic.
- **Modularity** â€” Itâ€™s easy to replace the mock logic with a real ML model or API call later.

---

## ðŸ”§ Mock Implementation Example

Hereâ€™s a simulation scaffold:

### 1. Sentiment Agent (Mock)

```python
from pydantic import BaseModel
import random

class SentimentInput(BaseModel):
    ticket_text: str

class SentimentOutput(BaseModel):
    sentiment: str  # "positive", "neutral", "negative"

class SentimentAgent:
    def run(self, input: SentimentInput) -> SentimentOutput:
        # Mock logic: choose random sentiment
        sentiment = random.choice(["positive", "neutral", "negative"])
        return SentimentOutput(sentiment=sentiment)
```

### 2. Priority Agent (Mock)

```python
class PriorityInput(BaseModel):
    customer_tier: str  # e.g., "gold", "silver", "free"
    ticket_urgency: int  # 1 to 5

class PriorityOutput(BaseModel):
    priority: str  # "high", "medium", "low"

class PriorityAgent:
    def run(self, input: PriorityInput) -> PriorityOutput:
        if input.customer_tier == "gold" or input.ticket_urgency >= 4:
            priority = "high"
        elif input.customer_tier == "silver":
            priority = "medium"
        else:
            priority = "low"
        return PriorityOutput(priority=priority)
```

### 3. Routing Agent (Mock)

```python
class RoutingInput(BaseModel):
    sentiment: str
    priority: str
    ticket_text: str

class RoutingOutput(BaseModel):
    route: str  # e.g., "technical", "billing", "retention"

class RoutingAgent:
    def run(self, input: RoutingInput) -> RoutingOutput:
        if "refund" in input.ticket_text.lower():
            route = "billing"
        elif input.sentiment == "negative" and input.priority == "high":
            route = "retention"
        else:
            route = "technical"
        return RoutingOutput(route=route)
```

---

### ðŸ§ª Sample Orchestrator

```python
class Orchestrator:
    def __init__(self):
        self.sentiment_agent = SentimentAgent()
        self.priority_agent = PriorityAgent()
        self.routing_agent = RoutingAgent()

    def route_ticket(self, ticket_text: str, customer_tier: str, ticket_urgency: int):
        sentiment = self.sentiment_agent.run(SentimentInput(ticket_text=ticket_text))
        priority = self.priority_agent.run(PriorityInput(
            customer_tier=customer_tier,
            ticket_urgency=ticket_urgency
        ))
        route = self.routing_agent.run(RoutingInput(
            sentiment=sentiment.sentiment,
            priority=priority.priority,
            ticket_text=ticket_text
        ))
        return {
            "sentiment": sentiment.sentiment,
            "priority": priority.priority,
            "route": route.route
        }
```

---

### ðŸ”„ Usage Example

```python
o = Orchestrator()
result = o.route_ticket(
    ticket_text="I want a refund, this product is broken!",
    customer_tier="gold",
    ticket_urgency=5
)
print(result)
```

Output (example):
```json
{
    "sentiment": "negative",
    "priority": "high",
    "route": "retention"
}
```

---

## ðŸ”„ Next Step After Prototype

Once your flow is stable, you can:
- Replace random/fake logic with real models (e.g., Hugging Face transformers, OpenAI API)
- Add logging, metrics, and test coverage
- Parallelize or async-execute agents if needed

Let me know if you want this scaffold as a runnable file or FastAPI app!
user
For the sentiment agent, I want it to:

1. Take subject + message

2. Output a sentiment score and urgency level

Can you help me sketch that out in Pydantic AI-style logic?
ChatGPT
Absolutely! Hereâ€™s a clean Pydantic AI-style design for your **Sentiment Agent**, incorporating:

- **Input**: `subject` and `message`  
- **Output**:  
  - `sentiment_score`: A float between -1.0 (very negative) and 1.0 (very positive)  
  - `urgency_level`: One of `"low"`, `"medium"`, or `"high"` (based on heuristics or score)

---

## ðŸ§  Sentiment Agent (Pydantic + Mock Logic)

### âœ… Step-by-step Goals:
- Define clear input/output schemas using `BaseModel`
- Simulate logic (mock rules or random)
- Keep it modular so it can plug into your orchestrator later

---

### ðŸ“¦ `sentiment_agent.py`

```python
from pydantic import BaseModel, Field
from typing import Literal
import random

# Input model
class SentimentInput(BaseModel):
    subject: str
    message: str

# Output model
class SentimentOutput(BaseModel):
    sentiment_score: float = Field(..., ge=-1.0, le=1.0)
    urgency_level: Literal["low", "medium", "high"]

# Agent logic
class SentimentAgent:
    def run(self, input: SentimentInput) -> SentimentOutput:
        full_text = f"{input.subject} {input.message}".lower()

        # ðŸ§ª Mock sentiment scoring logic
        if any(word in full_text for word in ["angry", "terrible", "hate", "refund", "cancel"]):
            score = random.uniform(-1.0, -0.3)
        elif any(word in full_text for word in ["thank", "great", "awesome", "love", "happy"]):
            score = random.uniform(0.3, 1.0)
        else:
            score = random.uniform(-0.2, 0.2)

        # ðŸ§  Rule-based urgency logic
        if score <= -0.5:
            urgency = "high"
        elif score <= 0:
            urgency = "medium"
        else:
            urgency = "low"

        return SentimentOutput(sentiment_score=round(score, 2), urgency_level=urgency)
```

---

### ðŸ§ª Example Usage

```python
agent = SentimentAgent()
result = agent.run(SentimentInput(
    subject="Extremely disappointed",
    message="This is the third time my account has been charged incorrectly. I want this resolved now!"
))

print(result)
```

Example Output:
```json
{
  "sentiment_score": -0.75,
  "urgency_level": "high"
}
```

---

## ðŸ§© Notes

- You can **easily upgrade** this logic to a sentiment model (e.g., `textblob`, `transformers`, or `OpenAI API`) later.
- The `urgency_level` is **rule-derived** here, but you can turn that into a classifier if needed.
- You could also extract other metadata (e.g., presence of exclamation marks, all-caps, complaint patterns) to refine this.

---

Let me know if you'd like this connected into the orchestrator, or rewritten with async logic or FastAPI.
user
Awesome. Now for the second agent,  I want it to calculate a "priority score" and "SLA target time" based on:

1. customer_tier (free, premium, enterprise)

2. previous_tickets

3. monthly_revenue

4. account_age_days

What would be a good way to mock this agent?
ChatGPT
Perfect â€” this is a great case for a second agent that **encodes business logic and SLAs**, even in mock form.

Letâ€™s break it down and build a mock **Priority Agent** that outputs:

### **Inputs**
- `customer_tier`: `"free"`, `"premium"`, `"enterprise"`
- `previous_tickets`: int (e.g. number of past tickets)
- `monthly_revenue`: float (USD or INR)
- `account_age_days`: int

### **Outputs**
- `priority_score`: A float from `0.0` to `1.0` â€” higher = more important
- `sla_target_time`: int (in hours) â€” how quickly support must respond

---

## ðŸ§  Heuristic Strategy for Mock Logic

- **customer_tier** is a strong driver of both priority and SLA
- More **monthly revenue** and older **account_age** â†’ higher priority
- Many **previous_tickets** â†’ maybe lower priority (spam), or higher (frustrated user) depending on context

We'll just go with **simple weights** for now.

---

## ðŸ“¦ `priority_agent.py`

```python
from pydantic import BaseModel, Field
from typing import Literal

# Input model
class PriorityInput(BaseModel):
    customer_tier: Literal["free", "premium", "enterprise"]
    previous_tickets: int
    monthly_revenue: float  # e.g. in USD
    account_age_days: int

# Output model
class PriorityOutput(BaseModel):
    priority_score: float = Field(..., ge=0.0, le=1.0)
    sla_target_time: int  # hours

class PriorityAgent:
    def run(self, input: PriorityInput) -> PriorityOutput:
        # Base score by tier
        tier_weights = {
            "free": 0.2,
            "premium": 0.6,
            "enterprise": 0.9
        }
        score = tier_weights[input.customer_tier]

        # Revenue influence
        score += min(input.monthly_revenue / 1000, 1.0) * 0.2  # max +0.2

        # Loyalty boost
        if input.account_age_days > 365:
            score += 0.1

        # Too many tickets? Slight penalty (spam-like)
        if input.previous_tickets > 10:
            score -= 0.05

        # Clamp score to 0.0 â€“ 1.0
        score = round(min(max(score, 0.0), 1.0), 2)

        # SLA by tier and score
        if score >= 0.85:
            sla = 2  # hours
        elif score >= 0.6:
            sla = 8
        elif score >= 0.3:
            sla = 24
        else:
            sla = 48

        return PriorityOutput(priority_score=score, sla_target_time=sla)
```

---

### ðŸ§ª Example Usage

```python
agent = PriorityAgent()
result = agent.run(PriorityInput(
    customer_tier="premium",
    previous_tickets=3,
    monthly_revenue=750.0,
    account_age_days=400
))

print(result)
```

Example Output:
```json
{
  "priority_score": 0.88,
  "sla_target_time": 2
}
```

---

## ðŸ§© Notes

- This agent is **fully mockable yet production-aligned** â€” easy to swap in real scoring models later.
- You can adjust weights, add more context (e.g. average sentiment from previous tickets), or even connect this to a DB or CRM system later.
- `sla_target_time` could later return datetime targets instead of hours, if needed.

---

Let me know if you want this connected with your Sentiment Agent + Routing Agent in a full working orchestrator!
user
Cool. Now, I thought of a third agent,  the routing agent - to use the outputs from the first two agents, and decide:

1. Which team the ticket should go to (Tier 1, Tier 2, Security, etc.)

2. Whether to escalate

Should I hardcode the routing logic or simulate it based on thresholds?
ChatGPT
Great question â€” and you're at a key decision point here.

The best approach for your **Routing Agent** at this prototyping stage is to use **simulated logic based on thresholds**, not hardcoded routes per specific combinations. Here's why:

---

## âœ… Why Use Threshold-Based Simulated Routing (Not Fully Hardcoded)

**Pros:**
- âœ¨ More flexible and realistic
- ðŸ§ª Easy to test edge cases by tweaking input scores
- ðŸ”„ Future-proof â€” real logic (like ML or rules engine) can replace thresholds without restructuring

**Hardcoding (like if sentiment is X and priority is Y then route to Z)** is too rigid and hard to scale.

---

## ðŸ§  Suggested Routing Logic (Mock Rules)

Inputs:
- `sentiment_score`: from Sentiment Agent
- `urgency_level`: from Sentiment Agent
- `priority_score`: from Priority Agent
- `sla_target_time`: from Priority Agent

### Output:
- `team`: `"Tier 1"`, `"Tier 2"`, `"Security"`, `"Billing"`, etc.
- `escalate`: `True` or `False`

---

## ðŸ“¦ `routing_agent.py`

```python
from pydantic import BaseModel
from typing import Literal

# Input model
class RoutingInput(BaseModel):
    sentiment_score: float  # -1.0 to 1.0
    urgency_level: Literal["low", "medium", "high"]
    priority_score: float  # 0.0 to 1.0
    sla_target_time: int
    ticket_text: str

# Output model
class RoutingOutput(BaseModel):
    team: Literal["Tier 1", "Tier 2", "Security", "Billing", "Retention"]
    escalate: bool

class RoutingAgent:
    def run(self, input: RoutingInput) -> RoutingOutput:
        text = input.ticket_text.lower()

        # Rule-based routing logic
        if "refund" in text or "billing" in text:
            team = "Billing"
        elif "password leak" in text or "unauthorized access" in text:
            team = "Security"
        elif input.priority_score >= 0.85 or input.urgency_level == "high":
            team = "Tier 2"
        else:
            team = "Tier 1"

        # Escalation logic (simulate threshold)
        escalate = (
            input.sentiment_score <= -0.6 and
            input.priority_score >= 0.7
        )

        return RoutingOutput(team=team, escalate=escalate)
```

---

## ðŸ§ª Example Usage

```python
agent = RoutingAgent()
result = agent.run(RoutingInput(
    sentiment_score=-0.8,
    urgency_level="high",
    priority_score=0.9,
    sla_target_time=2,
    ticket_text="My account was hacked and I see unauthorized charges!"
))

print(result)
```

Output:
```json
{
  "team": "Security",
  "escalate": true
}
```

---

## ðŸ§© Bonus: Extendability Ideas

- Later you can replace routing logic with a decision tree or rules engine (e.g., `Durable Rules`, `Experta`, or even a simple ML model).
- `escalate` can also return escalation **reason** (like `"VIP customer with low sentiment"`).

---

Let me know if you want to wire up all three agents in a complete orchestration flow â€” I can build that next!
user
Okay Iâ€™ve created those three agents using mock return values. Can you help me write a main.py that:
Loops through test tickets, Passes each ticket through the 3 agents, Aggregates the output into a dict and prints it?
ChatGPT
Absolutely! Here's a clean `main.py` that will:

1. Loop through test tickets (you can define them inline).
2. Pass each ticket through:
   - `SentimentAgent`
   - `PriorityAgent`
   - `RoutingAgent`
3. Aggregate all outputs into a single result dictionary per ticket.
4. Print the result in a readable format.

---

## âœ… `main.py`

```python
from sentiment_agent import SentimentAgent, SentimentInput
from priority_agent import PriorityAgent, PriorityInput
from routing_agent import RoutingAgent, RoutingInput

# ðŸ§ª Test tickets
test_tickets = [
    {
        "subject": "I was double charged!",
        "message": "Please fix this billing issue. I need a refund ASAP.",
        "customer_tier": "premium",
        "previous_tickets": 2,
        "monthly_revenue": 500.0,
        "account_age_days": 365
    },
    {
        "subject": "System hacked",
        "message": "My account had unauthorized login attempts. Very concerned.",
        "customer_tier": "enterprise",
        "previous_tickets": 1,
        "monthly_revenue": 3000.0,
        "account_age_days": 800
    },
    {
        "subject": "Feedback on product",
        "message": "Loving the new updates. Keep it up!",
        "customer_tier": "free",
        "previous_tickets": 0,
        "monthly_revenue": 0.0,
        "account_age_days": 30
    },
]

# ðŸŽ›ï¸ Initialize agents
sentiment_agent = SentimentAgent()
priority_agent = PriorityAgent()
routing_agent = RoutingAgent()

# ðŸ” Run through each ticket
for idx, ticket in enumerate(test_tickets, start=1):
    print(f"\n=== Processing Ticket #{idx} ===")

    # Step 1: Sentiment
    sentiment_result = sentiment_agent.run(SentimentInput(
        subject=ticket["subject"],
        message=ticket["message"]
    ))

    # Step 2: Priority
    priority_result = priority_agent.run(PriorityInput(
        customer_tier=ticket["customer_tier"],
        previous_tickets=ticket["previous_tickets"],
        monthly_revenue=ticket["monthly_revenue"],
        account_age_days=ticket["account_age_days"]
    ))

    # Step 3: Routing
    routing_result = routing_agent.run(RoutingInput(
        sentiment_score=sentiment_result.sentiment_score,
        urgency_level=sentiment_result.urgency_level,
        priority_score=priority_result.priority_score,
        sla_target_time=priority_result.sla_target_time,
        ticket_text=ticket["message"]
    ))

    # Step 4: Aggregate
    aggregated_result = {
        "subject": ticket["subject"],
        "sentiment_score": sentiment_result.sentiment_score,
        "urgency_level": sentiment_result.urgency_level,
        "priority_score": priority_result.priority_score,
        "sla_target_time": priority_result.sla_target_time,
        "assigned_team": routing_result.team,
        "escalate": routing_result.escalate
    }

    # Step 5: Print nicely
    for key, value in aggregated_result.items():
        print(f"{key.replace('_', ' ').title()}: {value}")
```

---

## ðŸ§© Output Sample

```
=== Processing Ticket #1 ===
Subject: I was double charged!
Sentiment Score: -0.73
Urgency Level: high
Priority Score: 0.78
Sla Target Time: 8
Assigned Team: Billing
Escalate: True
```

---

## ðŸ“¦ File Structure Suggestion

```
your_project/
â”œâ”€â”€ main.py
â”œâ”€â”€ sentiment_agent.py
â”œâ”€â”€ priority_agent.py
â””â”€â”€ routing_agent.py
```

---

Let me know if you want to export the results to a CSV, build a FastAPI wrapper, or visualize routing decisions!
user
Thanks, that worked.
Now I need to evaluate this system. The assignment wants at least 3 metrics across 5 test cases.
What metrics would make sense here?
ChatGPT
You're welcome â€” great progress so far! For evaluating a multi-agent ticket routing system, especially in a prototype/mock phase, you want **meaningful, interpretable metrics** that reflect:

- Accuracy or quality of routing decisions  
- Responsiveness to urgency  
- Priority alignment with business expectations  

---

## âœ… 3 Suggested Metrics (that make sense here)

### 1. **Routing Accuracy (Manual Evaluation)**
**Goal**: How often did the system route tickets to the "correct" team?

- Define a "ground truth" team for each test ticket.
- Compare against the `RoutingAgent`'s output.
- Metric: `% of correct routings`

```python
Routing Accuracy = (# Correctly Routed Tickets / Total Tickets) * 100
```

Even in mock systems, you can manually set expected values and compare.

---

### 2. **Escalation Precision**
**Goal**: Of the tickets marked as `escalate=True`, how many *actually deserved* escalation?

- Again, manually mark each test case with whether it **should** be escalated.
- Precision matters more than recall here â€” we want to avoid over-escalation.

```python
Escalation Precision = (True Positives / (True Positives + False Positives)) * 100
```

---

### 3. **SLA Compliance Risk**
**Goal**: Estimate how likely the ticket is to **miss SLA** based on urgency vs. target time.

Simulate a very rough heuristic:

- If `urgency_level == "high"` and `sla_target_time > 8` â†’ at-risk
- If `urgency_level == "medium"` and `sla_target_time > 24` â†’ at-risk
- Else, not at-risk

```python
SLA Risk Rate = (# At-Risk Tickets / Total Tickets) * 100
```

This tells you whether your priority agent is *underestimating urgency* or SLA needs.

---

## ðŸ§ª Optional Bonus Metrics (if you want to go deeper)

- **Priority Correlation**: Does `priority_score` correlate with `customer_tier` + `monthly_revenue`?
- **Sentimentâ€“Urgency Alignment**: Is low sentiment actually triggering `urgency_level = high`?
- **Team Distribution**: Are some teams overloaded due to logic bias? (e.g., everything routed to Tier 1)

---

## âœ… Final Recommendation

In your case study report, you can include this simple table format:

| Metric                 | Description                               | Value (%) |
|------------------------|-------------------------------------------|-----------|
| Routing Accuracy       | Correct team assignment                   | 80%       |
| Escalation Precision   | Escalations that were actually needed     | 100%      |
| SLA Risk Rate          | % of tickets with potential SLA failure   | 20%       |

And then describe:
- How you defined "ground truth"
- Any edge cases you noticed
- If the agent logic over- or under-reacted

---

Let me know if you want help generating a scoring function or CSV summary automatically from your `main.py` runs!
user
Great, Iâ€™ll create a metrics.py and I thought of implementing
Average sentiment score, Escalation count, Routing distribution
Iâ€™ll add one more for fun: high-priority tickets per tier.
ChatGPT
Perfect â€” those are **smart, meaningful metrics**, and very easy to compute from your aggregated results.

Hereâ€™s how you can structure `metrics.py` to include:

1. **Average Sentiment Score**
2. **Escalation Count**
3. **Routing Distribution**
4. **High-Priority Tickets per Customer Tier** (great insight)

---

## ðŸ“¦ Suggested `metrics.py`

```python
from collections import Counter, defaultdict
from typing import List, Dict, Any

def calculate_metrics(results: List[Dict[str, Any]], raw_tickets: List[Dict[str, Any]]):
    print("\n=== Evaluation Metrics ===")

    # 1. Average sentiment score
    avg_sentiment = round(sum(r["sentiment_score"] for r in results) / len(results), 2)
    print(f"Average Sentiment Score: {avg_sentiment}")

    # 2. Escalation count
    escalation_count = sum(1 for r in results if r["escalate"])
    print(f"Escalation Count: {escalation_count} / {len(results)}")

    # 3. Routing distribution
    routing_counts = Counter(r["assigned_team"] for r in results)
    print("\nRouting Distribution:")
    for team, count in routing_counts.items():
        print(f"  {team}: {count} tickets")

    # 4. High-priority tickets per customer tier
    tier_priority_map = defaultdict(int)
    for r, raw in zip(results, raw_tickets):
        if r["priority_score"] >= 0.75:
            tier = raw["customer_tier"]
            tier_priority_map[tier] += 1

    print("\nHigh-Priority Tickets per Customer Tier (priority_score â‰¥ 0.75):")
    for tier, count in tier_priority_map.items():
        print(f"  {tier}: {count} tickets")
```

---

## âœ… How to Use in `main.py`

At the end of `main.py`, after processing all tickets:

```python
from metrics import calculate_metrics

# After loop:
calculate_metrics(results=all_outputs, raw_tickets=test_tickets)
```

Assuming you stored outputs like this in your loop:
```python
all_outputs = []
for ticket in test_tickets:
    ...
    all_outputs.append(aggregated_result)
```

---

## ðŸ“Š Sample Output

```
=== Evaluation Metrics ===
Average Sentiment Score: -0.12
Escalation Count: 2 / 5

Routing Distribution:
  Billing: 1 tickets
  Security: 1 tickets
  Tier 2: 2 tickets
  Tier 1: 1 tickets

High-Priority Tickets per Customer Tier (priority_score â‰¥ 0.75):
  premium: 1 tickets
  enterprise: 2 tickets
```

---

Let me know if you want to export the results and metrics into a report, JSON, or CSV file!